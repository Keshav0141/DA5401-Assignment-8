# DA5401-Assignment-8 â€” Ensemble Learning for Complex Regression Modeling on Bike Share Data

**Name:** Aryan Prasad  
**Roll No:** DA25M007  

---

## ğŸ§  Overview
This assignment applies and compares three ensemble learning techniques â€” **Bagging**, **Boosting**, and **Stacking** â€” for predicting hourly bike rental counts (`cnt`) from the **UCI Bike Sharing Demand Dataset**.  
The objective is to analyze how each method tackles the **biasâ€“variance trade-off** and improves predictive accuracy over single models using **Root Mean Squared Error (RMSE)** as the evaluation metric.

---

## ğŸš² Dataset Description
**Source:** UCI Machine Learning Repository  
**Samples:** 17 379 (hourly)â€ƒ**Target:** `cnt` (Total rentals)  

**Features**
- **Time-based:** `yr`, `mnth`, `hr`, `weekday`, `holiday`, `workingday`  
- **Weather-related:** `temp`, `atemp`, `hum`, `windspeed`  
- **Categorical:** `season`, `weathersit`  

**Dropped Columns:** `instant`, `dteday`, `casual`, `registered` (redundant / leakage)

---

## âš™ï¸ Workflow Summary
| Part | Focus | Key Methods |
|:----:|:------|:------------|
| A | Preprocessing & Baseline | Linear Regression and Decision Tree (max_depth = 6) |
| B | Ensemble Methods | Bagging (variance reduction) & Gradient Boosting (bias reduction) |
| C | Stacking | KNN + Bagging + GB with Ridge meta-learner |
| D | Final Analysis | RMSE comparison & biasâ€“variance interpretation |

---

## ğŸ“Š Results
| Model | RMSE |
|:--------------------------------------------|:-----------:|
| Baseline (best of DT / Linear Regression) | 133.84 |
| Bagging Regressor | 155.48 |
| Gradient Boosting Regressor | 122.45 |
| **Stacking Regressor (KNN + Bagging + GB â†’ Ridge)** | **114.17** |

**ğŸ† Best Model:** Stacking Regressor  

---

## ğŸ” Insights
- **Baseline Models:** Linear Regression > Decision Tree â†’ baseline RMSE 133.84.  
- **Bagging:** Reduced variance but minor accuracy gain.  
- **Boosting:** Corrected bias â†’ major improvement (RMSE 122.45).  
- **Stacking:** Blended diverse models via Ridge meta-learner â†’ best generalization (RMSE 114.17).  

---

## âš–ï¸ Biasâ€“Variance Summary
| Model | Bias | Variance | Comment |
|:------|:-----:|:---------:|:--------|
| Decision Tree | Low | High | Overfits |
| Bagging | â‰ˆ Low | â†“ Variance | Stabilizes predictions |
| Boosting | â†“ Bias | Slight â†‘ Variance | Captures non-linearity |
| Stacking | â†“ Bias | â†“ Variance | Best balance |

---

## ğŸ“ˆ Visual Highlights
- **Correlation Heatmap** â€” Feature relationships with `cnt`.  
- **Hourly Trend Plot** â€” Morning & evening rental peaks.  
- **Distribution Plot** â€” Right-skewed rental frequency.  
- **Actual vs Predicted** â€” Side-by-side for Bagging & Boosting.  
- **RMSE Trend Plot** â€” Shows steady improvement to Stacking.  
- **Error Distribution** â€” Stacking errors narrower and centered around 0.  

---

## ğŸ§  Key Interpretation
- **Bagging** â†“ variance.â€ƒ**Boosting** â†“ bias.â€ƒ**Stacking** balances both.  
- **Ridge Meta-Learner** adds L2 regularization to avoid overfitting.  
- Ensemble methods show clear superiority over single models in handling non-linearity and feature interaction.

---

## âœ… Final Conclusion
**Stacking Regressor achieved the lowest RMSE (114.17)** â€” â‰ˆ 15 % improvement over the baseline.  
By combining KNN, Bagging, and Gradient Boosting with a Ridge meta-learner, it achieved optimal biasâ€“variance trade-off and the most accurate bike-demand predictions.

---

